{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thêm thư viện cần thiết để sử dụng tính toán\n",
    "\n",
    "- `import math` được sử dụng trong tính toán liên quan đến toán.\n",
    "\n",
    "- `import random` được sử dụng để tạo số ngẫu nhiên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dữ liệu huấn luyện\n",
    "X = [[1, 1], [2, 2], [2, 3], [3, 3]]  # Đầu vào\n",
    "y_true = [2, 4, 5, 6]  # Đầu ra mục tiêu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thêm dữ liệu input và giá trị mong muốn mô hình dự đoán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm kích hoạt sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng hàm kích hoạt Sigmoid và đạo hàm Sigmoid (dùng trong tính toán Gradient)\n",
    "\n",
    "  - Công thức Sigmoid: $$y_{pred}(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "  - Đạo hàm Sigmoid: $$y_{pred}(1-y_{pred})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm chuẩn hóa và khôi phục giá trị\n",
    "def normalize(data, min_val, max_val):\n",
    "    return [(d - min_val) / (max_val - min_val) for d in data]\n",
    "\n",
    "def denormalize(data, min_val, max_val):\n",
    "    return [d * (max_val - min_val) + min_val for d in data]\n",
    "\n",
    "\n",
    "# Chuẩn hóa dữ liệu\n",
    "x_min = min(min(row) for row in X)\n",
    "x_max = max(max(row) for row in X)\n",
    "X_normalized = [[(xi - x_min) / (x_max - x_min) for xi in row] for row in X]\n",
    "\n",
    "y_min, y_max = min(y_true), max(y_true)\n",
    "y_true_normalized = normalize(y_true, y_min, y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng các hàm và thao tác chuẩn hóa dữ liệu. \n",
    "\n",
    "  - Do giá trị output $y_{pred}$ của các nơ-ron có giá trị thuộc khoảng (0,1) nên nếu không chuẩn hóa dữ liệu huấn luyện thì sẽ đẫn đến sai sót làm mô hình không thể dự đoán chính xác.\n",
    "\n",
    "  - Cơ chế hàm chuyển hóa là tùy người lập trình viên thiết lập.\n",
    "\n",
    "  - Hàm **normalize** được dùng để chuẩn hóa dữ liệu thành giá trị thuộc khoảng (0,1).\n",
    "\n",
    "  - Hàm **denormalize** được dùng để chuẩn hóa dữ liệu từ các giá trị thuộc khoảng (0,1) thành giá trị dữ liệu ban đầu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo trọng số và bias\n",
    "random.seed(42)\n",
    "weights_input_hidden = [[random.uniform(-0.5, 0.5) for _ in range(2)] for _ in range(4)]\n",
    "bias_hidden = [random.uniform(-0.5, 0.5) for _ in range(4)]\n",
    "weights_hidden_output = [random.uniform(-0.5, 0.5) for _ in range(4)]\n",
    "bias_output = random.uniform(-0.5, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Khởi tạo các trọng số $w$ và hệ số bias ban đầu một cách ngẫu nhiên (sử dụng random)\n",
    "\n",
    "- `random.seed(42)`\n",
    "\n",
    "  - Đặt giá trị seed cho random, đảm bảo ở các lần khởi chạy lại chương trình thì kết quả là giống nhau.\n",
    "\n",
    "- `weights_input_hidden = [[random.uniform(-0.5, 0.5) for _ in range(2)] for _ in range(4)]` và `bias_hidden = [random.uniform(-0.5, 0.5) for _ in range(4)]`\n",
    "\n",
    "  - Khởi tạo các trọng số $w$ cho dữ liệu đầu vào input.\n",
    "\n",
    "  - Như đã nói ở trên, do mỗi dữ liệu đầu vào $x$ có dạng là một mảng gồm 2 phần tử $x=[value_1,value_2]$ nên các giá trị trọng số cũng phải có dạng như vậy.\n",
    "\n",
    "  - Mảng weights_input_hidden sẽ có dạng: $[ [w_{11},w_{12}] , [w_{21},w_{22}] , [w_{31},w_{32}] , [w_{41},w_{41}] ]$.\n",
    "\n",
    "  - Mảng bias_hidden sẽ có dạng: $[b_1,b_2,b_3,b_4]$.\n",
    "\n",
    "- `weights_hidden_output = [random.uniform(-0.5, 0.5) for _ in range(4)]` và `bias_output = random.uniform(-0.5, 0.5)`\n",
    "\n",
    "  - Khởi tạo các giá trị trọng số $w$ và hệ số bias $b$ ngẫu nhiên cho lớp layer output.\n",
    "\n",
    "  - Do giá trị đầu ra của nơ-ron thuộc lớp trước đó lần lượt là 4 giá trị đơn $y_{pred-1},y_{pred-2},y_{pred-3},y_{pred-4}$ (không còn là một mảng nữa) nên trọng số đầu ra sẽ có dạng $[w_1,w_2,w_3,w_4]$.\n",
    "\n",
    "- **Lưu ý**: do chỉ thiết kế một lớp nơ-ron thuộc Hidden Layers nên bên trên chỉ khởi tạo một **weights_input_hidden** và **bias_hidden**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tốc độ học\n",
    "learning_rate = 0.02\n",
    "epochs = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Khai báo tốc độ học `learning_rate` và số lần học của mô hình `epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss: 0.0614, Predictions: [3.4724330020366043, 4.214520556361539, 4.5676393319824555, 4.772742213383027]\n",
      "Epoch 2000, Loss: 0.0154, Predictions: [2.6449261486333224, 4.1712650534734745, 4.937632353056188, 5.269847257276039]\n",
      "Epoch 3000, Loss: 0.0073, Predictions: [2.3818132365214333, 4.090054564312527, 5.071551495749529, 5.446585540422481]\n",
      "Epoch 4000, Loss: 0.0049, Predictions: [2.280291708869563, 4.030951798002743, 5.132541400131819, 5.533868190226403]\n",
      "Epoch 5000, Loss: 0.0039, Predictions: [2.2303330591194954, 3.994044928748269, 5.16460635407649, 5.587287996632498]\n",
      "Epoch 6000, Loss: 0.0034, Predictions: [2.201783207187383, 3.971352228802693, 5.18062648954926, 5.623581753318507]\n",
      "Epoch 7000, Loss: 0.0030, Predictions: [2.183809133217428, 3.957494126379104, 5.1865805984802975, 5.650113197034276]\n",
      "Epoch 8000, Loss: 0.0027, Predictions: [2.1716850684371285, 3.9492963054830907, 5.185929785118779, 5.670730508688606]\n",
      "Epoch 9000, Loss: 0.0025, Predictions: [2.163044993442719, 3.9448707704399726, 5.180893440672758, 5.687620879025209]\n",
      "Epoch 10000, Loss: 0.0023, Predictions: [2.1565769038756857, 3.9430454063064753, 5.172984539447977, 5.702085195232837]\n",
      "Epoch 11000, Loss: 0.0021, Predictions: [2.1514950383106717, 3.9430498888839005, 5.163280513867984, 5.714915199797986]\n",
      "Epoch 12000, Loss: 0.0019, Predictions: [2.1473018984157175, 3.9443456766522855, 5.152571973378169, 5.726596102466546]\n",
      "Epoch 13000, Loss: 0.0018, Predictions: [2.14367145841919, 3.9465344664524022, 5.141447325819208, 5.737423548244662]\n",
      "Epoch 14000, Loss: 0.0016, Predictions: [2.1403874346951155, 3.949309652825362, 5.13034251308, 5.747574901903628]\n",
      "Epoch 15000, Loss: 0.0015, Predictions: [2.1373079873742604, 3.952431091618416, 5.119572335078411, 5.757154353348536]\n",
      "Epoch 16000, Loss: 0.0014, Predictions: [2.1343436094991817, 3.9557117544926657, 5.109352814433013, 5.766222037567802]\n",
      "Epoch 17000, Loss: 0.0012, Predictions: [2.1314419190396765, 3.9590096768701417, 5.099819584039551, 5.774812871886208]\n",
      "Epoch 18000, Loss: 0.0011, Predictions: [2.128576421550035, 3.9622216948533215, 5.091044402095806, 5.782948540962597]\n",
      "Epoch 19000, Loss: 0.0011, Predictions: [2.125737926174962, 3.965277468800394, 5.083050257208885, 5.790644845622554]\n",
      "Epoch 20000, Loss: 0.0010, Predictions: [2.1229280138562823, 3.968133457760776, 5.075824818227716, 5.797915931285495]\n",
      "Epoch 21000, Loss: 0.0009, Predictions: [2.1201542182553617, 3.9707670466272478, 5.069331868950247, 5.80477646685053]\n",
      "Epoch 22000, Loss: 0.0008, Predictions: [2.1174266424798294, 3.973171159205479, 5.063520535166198, 5.811242534744128]\n",
      "Epoch 23000, Loss: 0.0008, Predictions: [2.114755737007588, 3.9753496161549187, 5.058332342611612, 5.8173317632060035]\n",
      "Epoch 24000, Loss: 0.0007, Predictions: [2.112150968411822, 3.97731335872977, 5.053706326797607, 5.823063058806156]\n",
      "Epoch 25000, Loss: 0.0007, Predictions: [2.1096201318832746, 3.97907753450874, 5.049582515645376, 5.828456168675336]\n",
      "Epoch 26000, Loss: 0.0007, Predictions: [2.1071690996136865, 3.9806593581494916, 5.04590413253076, 5.833531209825412]\n",
      "Epoch 27000, Loss: 0.0006, Predictions: [2.104801842520146, 3.9820766192777564, 5.042618844828534, 5.838308239959604]\n",
      "Epoch 28000, Loss: 0.0006, Predictions: [2.102520606500386, 3.9833467003224827, 5.039679335038705, 5.842806903461014]\n",
      "Epoch 29000, Loss: 0.0005, Predictions: [2.1003261617024784, 3.984485977097715, 5.037043415462572, 5.8470461616585165]\n",
      "Epoch 30000, Loss: 0.0005, Predictions: [2.0982180724688417, 3.9855094942196336, 5.034673853673826, 5.851044102914331]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in zip(X_normalized, y_true_normalized):\n",
    "        # Lan truyền xuôi\n",
    "        hidden_inputs = [sum(w * xi for w, xi in zip(weights, x)) + b for weights, b in zip(weights_input_hidden, bias_hidden)]\n",
    "        \n",
    "        hidden_outputs = [sigmoid(h) for h in hidden_inputs]\n",
    "        \n",
    "        output_input = sum(w * h for w, h in zip(weights_hidden_output, hidden_outputs)) + bias_output\n",
    "        \n",
    "        output = sigmoid(output_input)\n",
    "\n",
    "        # Tính lỗi\n",
    "        loss = (y - output) ** 2\n",
    "        \n",
    "        total_loss += loss\n",
    "\n",
    "        # Lan truyền ngược\n",
    "        output_error = 2 * (output - y) * sigmoid_derivative(output_input)\n",
    "        \n",
    "        grad_weights_hidden_output = [output_error * h for h in hidden_outputs]\n",
    "        \n",
    "        grad_bias_output = output_error\n",
    "\n",
    "        \n",
    "        hidden_errors = [output_error * w * sigmoid_derivative(h) for w, h in zip(weights_hidden_output, hidden_inputs)]\n",
    "        \n",
    "        grad_weights_input_hidden = [[he * xi for xi in x] for he in hidden_errors]\n",
    "        \n",
    "        grad_bias_hidden = hidden_errors\n",
    "\n",
    "        # Cập nhật trọng số và bias\n",
    "        weights_hidden_output = [w - learning_rate * gw for w, gw in zip(weights_hidden_output, grad_weights_hidden_output)]\n",
    "        bias_output -= learning_rate * grad_bias_output\n",
    "        \n",
    "        for i in range(4):\n",
    "            weights_input_hidden[i] = [w - learning_rate * gw for w, gw in zip(weights_input_hidden[i], grad_weights_input_hidden[i])]\n",
    "            bias_hidden[i] -= learning_rate * grad_bias_hidden[i]\n",
    "\n",
    "    # In lỗi sau mỗi 1000 epoch\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        predictions = []\n",
    "        for x in X_normalized:\n",
    "            hidden_inputs = [sum(w * xi for w, xi in zip(weights, x)) + b for weights, b in zip(weights_input_hidden, bias_hidden)]\n",
    "            hidden_outputs = [sigmoid(h) for h in hidden_inputs]\n",
    "            output_input = sum(w * h for w, h in zip(weights_hidden_output, hidden_outputs)) + bias_output\n",
    "            output = sigmoid(output_input)\n",
    "            prediction = denormalize([output], y_min, y_max)[0]\n",
    "            predictions.append(prediction)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(X):.4f}, Predictions: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giải thích từng phần trong vòng lặp huấn luyện mô hình:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `for epoch in range(epochs):`\n",
    "\n",
    "  - Vòng lặp huấn luyện mô hình (số lần mà mô hình thực hiện lan truyền xuôi và lan truyền ngược để cập nhật trọng số).\n",
    "\n",
    "- `total_loss = 0`\n",
    "\n",
    "  - Biến được dùng để tính tổng giá trị của hàm mất mát sau 1000 lần học."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in zip(X_normalized, y_true_normalized):\n",
    "           pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `for x, y in zip(X_normalized, y_true_normalized):`\n",
    "\n",
    "  - Sử dụng zip để lần lượt gọi các phần tử bên trong mảng X_normalized - mảng chứa giá trị chuẩn hóa của input đầu vào; và y_true_normalized - mảng chứa các giá trị chuẩn hóa của output mong đợi.\n",
    "\n",
    "  - 2 mảng đã được khởi tạo trước đó. [Xem](#khởi-tạo-các-trọng-số--và-hệ-số-bias-ban-đầu-một-cách-ngẫu-nhiên-sử-dụng-random)\n",
    "\n",
    "- Bên dưới đây là các bước thực hiện trong **một lần học** của mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.327663009168956, 0.8718438627004594, -1.6726036719769615, -0.28216647745914164]\n",
      "[-0.3114636004441813, -0.42918582090568536, -0.09331510060380377, 0.4616524996507629]\n",
      "[-0.974953113224736, -0.8358772066799258, 0.2928007609893408, 0.4882930409359851]\n",
      "[-1.9505902100573187, -1.7302155045118301, 1.485973470769354, 1.2054714767606673]\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(X_normalized, y_true_normalized):\n",
    "        \n",
    "       # Lan truyền xuôi\n",
    "       hidden_inputs = [sum(w * xi for w, xi in zip(weights, x)) + b for weights, b in zip(weights_input_hidden, bias_hidden)]\n",
    "        \n",
    "       hidden_outputs = [sigmoid(h) for h in hidden_inputs]\n",
    "        \n",
    "       output_input = sum(w * h for w, h in zip(weights_hidden_output, hidden_outputs)) + bias_output\n",
    "        \n",
    "       output = sigmoid(output_input)\n",
    "       \n",
    "       print(hidden_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Xây dựng lan truyền xuôi**\n",
    "\n",
    "  - `hidden_inputs`: Là mảng chứa các giá trị đầu ra $y$ của hàm tính toán giá trị đầu vào bên trong nơ-ron. $[y_1,y_2,y_3,y_4]$\n",
    "\n",
    "    - Các phần tử trong mảng sẽ được tính lần lượt theo công thức của hàm tính toán đầu vào được nhắc bên trên.\n",
    "\n",
    "    - Các phần tử trong mảng sẽ được tính như sau: \n",
    "    \n",
    "      $y_1$ = weights_input_hidden[0][0] $\\times$ X_normalized[0][0] + weights_input_hidden[0][1] $\\times$ X_ normalized[0][1] + bias_hidden[0] \n",
    "\n",
    "      $y_2$ = weights_input_hidden[1][0] $\\times$ X_normalized[1][0] + weights_input_hidden[1][1] $\\times$ X_ normalized[1][1] + bias_hidden[1]\n",
    "\n",
    "      $y_3$ = weights_input_hidden[2][0] $\\times$ X_normalized[2][0] + weights_input_hidden[2][1] $\\times$ X_ normalized[2][1] + bias_hidden[2]\n",
    "\n",
    "      $y_4$ = weights_input_hidden[3][0] $\\times$ X_normalized[3][0] + weights_input_hidden[3][1] $\\times$ X_ normalized[3][1] + bias_hidden[3]\n",
    "\n",
    "    - Như vậy là lớp nơ-ron này sẽ có 4 nơ-ron bên trong và mỗi nơ-ron nhận một giá trị input $x$.\n",
    "\n",
    "  - `hidden_output`: Là mảng chứa các giá trị đầu ra $y_{pred}$ của các nơ-ron sau khi các giá trị $y$ bên trên được đi qua hàm kích hoạt Sigmoid. $[y_{pred-1},y_{pred-2},y_{pred-3},y_{pred-4}]$\n",
    "\n",
    "    - Các phần tử trong mảng sẽ được tính như sau: \n",
    "      \n",
    "      $y_{pred-1}$ = Sigmoid($y_1$)\n",
    "\n",
    "      $y_{pred-2}$ = Sigmoid($y_2$)\n",
    "\n",
    "      $y_{pred-3}$ = Sigmoid($y_3$)\n",
    "\n",
    "      $y_{pred-4}$ = Sigmoid($y_4$)\n",
    "\n",
    "  - `output_input`: Là biến chứa các giá trị đầu ra $o$ của hàm tính toán giá trị đầu vào bên trong nơ-ron. Đây là nơ-ron trong lớp Ouput Layer.\n",
    "\n",
    "    - Giá trị $o$ sẽ được tính toán như sau: \n",
    "\n",
    "      $o$ = weights_hidden_output[0] $\\times$ hidden_outputs[0] + weights_hidden_output[1] $\\times$ hidden_outputs[1] + weights_hidden_output[2] $\\times$ hidden_outputs[2] + weights_hidden_output[3] $\\times$ hidden_outputs[3] + bias_output\n",
    "\n",
    "    - Như vậy ta thấy lớp **Output Layer chỉ có một nơ-ron trong lớp này và nơ-ron này nhận 4 giá trị input đầu vào** $[y_{pred-1},y_{pred-2},y_{pred-3},y_{pred-4}]$.\n",
    "\n",
    "  - `output`: Là biến lưu kết quả output đầu ra $o_{pred}$ của nơ-ron thuộc lớp Output Layers. Sau khi $o$ đi qua hàm kích hoạt Sigmoid.\n",
    "\n",
    "    $o_{pred}$ = Sigmoid($o$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(X_normalized, y_true_normalized):\n",
    "       \n",
    "       # Lan truyền xuôi\n",
    "       hidden_inputs = [sum(w * xi for w, xi in zip(weights, x)) + b for weights, b in zip(weights_input_hidden, bias_hidden)]\n",
    "        \n",
    "       hidden_outputs = [sigmoid(h) for h in hidden_inputs]\n",
    "        \n",
    "       output_input = sum(w * h for w, h in zip(weights_hidden_output, hidden_outputs)) + bias_output\n",
    "        \n",
    "       output = sigmoid(output_input)\n",
    "\n",
    "       # Tính lỗi\n",
    "       loss = (y - output) ** 2\n",
    "        \n",
    "       total_loss += loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tính toán hàm lỗi**\n",
    "\n",
    "  - `loss = (y - output) ** 2` \n",
    "\n",
    "    - Là biến lưu kết quả tính lỗi dựa trên kết quả đầu ra của Output Layer với giá trị mà ta mong đợi mô hình dự đoán. \n",
    "  \n",
    "    - Sử dụng công thức hàm mất mát $$L (Loss) = \\frac{1}{n} \\sum_{i=1}^{n} (y_{true,i} - y_{pred,i})^2$$ \n",
    "\n",
    "    - Do ở đây ta chỉ tính tổng chứ không tính trung bình nên không chia cho $n$.\n",
    "\n",
    "  - `total_loss += loss`\n",
    "\n",
    "    - Là biến tính tổng lỗi sau một 1000 lần học."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(X_normalized, y_true_normalized):\n",
    "        # Lan truyền xuôi\n",
    "        hidden_inputs = [sum(w * xi for w, xi in zip(weights, x)) + b for weights, b in zip(weights_input_hidden, bias_hidden)]\n",
    "        \n",
    "        hidden_outputs = [sigmoid(h) for h in hidden_inputs]\n",
    "        \n",
    "        output_input = sum(w * h for w, h in zip(weights_hidden_output, hidden_outputs)) + bias_output\n",
    "        \n",
    "        output = sigmoid(output_input)\n",
    "\n",
    "        # Tính lỗi\n",
    "        loss = (y - output) ** 2\n",
    "        \n",
    "        total_loss += loss\n",
    "\n",
    "        # Lan truyền ngược\n",
    "        output_error = 2 * (output - y) * sigmoid_derivative(output_input)\n",
    "        \n",
    "        grad_weights_hidden_output = [output_error * h for h in hidden_outputs]\n",
    "        \n",
    "        grad_bias_output = output_error\n",
    "\n",
    "        \n",
    "        hidden_errors = [output_error * w * sigmoid_derivative(h) for w, h in zip(weights_hidden_output, hidden_inputs)]\n",
    "        \n",
    "        grad_weights_input_hidden = [[he * xi for xi in x] for he in hidden_errors]\n",
    "        \n",
    "        grad_bias_hidden = hidden_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tính lan truyền ngược và thực hiện cập nhật trọng số**\n",
    "\n",
    "  - Ta có các công thức liên quan\n",
    "\n",
    "    - Gradient của hàm $L$ với trọng số $w_i$: $$\\frac{\\partial L}{\\partial w_i} = \\frac{2}{n} (y_{pred} - y_{true}) \\times y_{pred}(1-y_{pred}) \\times x_i$$\n",
    "\n",
    "    - Gradient của hàm $L$ với hệ số bias: $$\\frac{\\partial L}{\\partial w_0} = \\frac{2}{n} (y_{pred} - y_{true}) \\times y_{pred}(1-y_{pred})$$\n",
    "  \n",
    "  - `output_error`\n",
    "\n",
    "    - Là biến được dùng để lưu giá trị của phép tính $\\frac{2}{n} (y_{pred} - y_{true}) \\times y_{pred}(1-y_{pred})$\n",
    "\n",
    "    - Ở đây, ta không chia cho n do sử dụng thuật toán tối ưu SGD (chỉ cập nhật trên một mẫu riêng lẻ do đang trong vòng lặp for từng tập dữ liệu $x$ trong tập input đầu vào)\n",
    "\n",
    "    - Nên `output_error` = ${2} \\times (o_{pred} - y_{true}) \\times o_{pred}(1-o_{pred})$\n",
    "\n",
    "  - `grad_weights_hidden_output`\n",
    "\n",
    "    - Là biến được dùng để lưu **giá trị tính Gradient của hàm mất mát so với các trọng số trong lớp nơ-ron của Output Layer** $\\frac{\\partial L}{\\partial w_i}$\n",
    "\n",
    "  - `grad_bias_output`\n",
    "\n",
    "    - Là biến được dùng để lưu **giá trị tính Gradient của hàm mất mát so với hệ số bias trong lớp nơ-ron của Output Layer** $\\frac{\\partial L}{\\partial w_0}$\n",
    "\n",
    "  - `hidden_errors`\n",
    "\n",
    "    - Công dụng tương tự với `output_error`.\n",
    "\n",
    "  - `grad_weights_input_hidden`\n",
    "\n",
    "    - Mảng dùng để lưu **giá trị tính Gradient của hàm mất mát so với các trọng số trong lớp nơ-ron của Hidden Layer** $\\frac{\\partial L}{\\partial w_i}$\n",
    "\n",
    "  - `grad_bias_hidden`\n",
    "\n",
    "    - Là biến được dùng để lưu **giá trị tính Gradient của hàm mất mát so với hệ số bias trong lớp nơ-ron của Hidden Layer** $\\frac{\\partial L}{\\partial w_0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(X_normalized, y_true_normalized):\n",
    "        # Lan truyền xuôi\n",
    "        hidden_inputs = [sum(w * xi for w, xi in zip(weights, x)) + b for weights, b in zip(weights_input_hidden, bias_hidden)]\n",
    "        \n",
    "        hidden_outputs = [sigmoid(h) for h in hidden_inputs]\n",
    "        \n",
    "        output_input = sum(w * h for w, h in zip(weights_hidden_output, hidden_outputs)) + bias_output\n",
    "        \n",
    "        output = sigmoid(output_input)\n",
    "\n",
    "        # Tính lỗi\n",
    "        loss = (y - output) ** 2\n",
    "        \n",
    "        total_loss += loss\n",
    "\n",
    "        # Lan truyền ngược\n",
    "        output_error = 2 * (output - y) * sigmoid_derivative(output_input)\n",
    "        \n",
    "        grad_weights_hidden_output = [output_error * h for h in hidden_outputs]\n",
    "        \n",
    "        grad_bias_output = output_error\n",
    "\n",
    "        \n",
    "        hidden_errors = [output_error * w * sigmoid_derivative(h) for w, h in zip(weights_hidden_output, hidden_inputs)]\n",
    "        \n",
    "        grad_weights_input_hidden = [[he * xi for xi in x] for he in hidden_errors]\n",
    "        \n",
    "        grad_bias_hidden = hidden_errors\n",
    "\n",
    "        # Cập nhật trọng số và bias\n",
    "        weights_hidden_output = [w - learning_rate * gw for w, gw in zip(weights_hidden_output, grad_weights_hidden_output)]\n",
    "        bias_output -= learning_rate * grad_bias_output\n",
    "        \n",
    "        for i in range(4):\n",
    "            weights_input_hidden[i] = [w - learning_rate * gw for w, gw in zip(weights_input_hidden[i], grad_weights_input_hidden[i])]\n",
    "            bias_hidden[i] -= learning_rate * grad_bias_hidden[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cập nhật trọng số $w$ và hệ số bias\n",
    "\n",
    "  - `weights_hidden_output` và `bias_output`\n",
    "\n",
    "    - Cập nhật trọng số và hệ số bias cho lơp nơ-ron trong Output Layer.\n",
    "\n",
    "  - `weights_input_hidden` và `bias_hidden`\n",
    "\n",
    "    - Cập nhật các trọng số và hệ số bias cho lớp nơ-ron trong Hidden Layer, được xử lý trong vòng lặp for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(X_normalized, y_true_normalized):\n",
    "        # Lan truyền xuôi\n",
    "        hidden_inputs = [sum(w * xi for w, xi in zip(weights, x)) + b for weights, b in zip(weights_input_hidden, bias_hidden)]\n",
    "        \n",
    "        hidden_outputs = [sigmoid(h) for h in hidden_inputs]\n",
    "        \n",
    "        output_input = sum(w * h for w, h in zip(weights_hidden_output, hidden_outputs)) + bias_output\n",
    "        \n",
    "        output = sigmoid(output_input)\n",
    "\n",
    "        # Tính lỗi\n",
    "        loss = (y - output) ** 2\n",
    "        \n",
    "        total_loss += loss\n",
    "\n",
    "        # Lan truyền ngược\n",
    "        output_error = 2 * (output - y) * sigmoid_derivative(output_input)\n",
    "        \n",
    "        grad_weights_hidden_output = [output_error * h for h in hidden_outputs]\n",
    "        \n",
    "        grad_bias_output = output_error\n",
    "\n",
    "        \n",
    "        hidden_errors = [output_error * w * sigmoid_derivative(h) for w, h in zip(weights_hidden_output, hidden_inputs)]\n",
    "        \n",
    "        grad_weights_input_hidden = [[he * xi for xi in x] for he in hidden_errors]\n",
    "        \n",
    "        grad_bias_hidden = hidden_errors\n",
    "\n",
    "        # Cập nhật trọng số và bias\n",
    "        weights_hidden_output = [w - learning_rate * gw for w, gw in zip(weights_hidden_output, grad_weights_hidden_output)]\n",
    "        bias_output -= learning_rate * grad_bias_output\n",
    "        \n",
    "        for i in range(4):\n",
    "            weights_input_hidden[i] = [w - learning_rate * gw for w, gw in zip(weights_input_hidden[i], grad_weights_input_hidden[i])]\n",
    "            bias_hidden[i] -= learning_rate * grad_bias_hidden[i]\n",
    "\n",
    "    # In lỗi sau mỗi 1000 epoch\n",
    "if (epoch + 1) % 1000 == 0:\n",
    "        predictions = []\n",
    "        for x in X_normalized:\n",
    "            hidden_inputs = [sum(w * xi for w, xi in zip(weights, x)) + b for weights, b in zip(weights_input_hidden, bias_hidden)]\n",
    "            hidden_outputs = [sigmoid(h) for h in hidden_inputs]\n",
    "            output_input = sum(w * h for w, h in zip(weights_hidden_output, hidden_outputs)) + bias_output\n",
    "            output = sigmoid(output_input)\n",
    "            prediction = denormalize([output], y_min, y_max)[0]\n",
    "            predictions.append(prediction)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(X):.4f}, Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In các thông tin liên quan đến các giá trị output của mô hình và tổng lỗi sau mỗi 1000 lần học."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra kết quả\n",
    "print(\"\\nFinal Predictions:\")\n",
    "for x,x_input in zip(X_normalized,X):\n",
    "    hidden_inputs = [sum(w * xi for w, xi in zip(weights, x)) + b for weights, b in zip(weights_input_hidden, bias_hidden)]\n",
    "    hidden_outputs = [sigmoid(h) for h in hidden_inputs]\n",
    "    output_input = sum(w * h for w, h in zip(weights_hidden_output, hidden_outputs)) + bias_output\n",
    "    output = sigmoid(output_input)\n",
    "    prediction = denormalize([output], y_min, y_max)[0]\n",
    "    print(f\"Input: {x_input}, Predicted Output: {prediction:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sau khi đã kết thúc số lần học mà ta thiết lập, ta tiến hành kiểm tra dữ liệu output của mô hình xem có gần với giá trị mà ta mong đợi không"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
